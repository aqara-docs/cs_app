import streamlit as st
import requests
import base64
import requests
import re
import datetime
from bs4 import BeautifulSoup
import os
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from konlpy.tag import Okt
from collections import Counter
import re

import os
from dotenv import load_dotenv
load_dotenv()

# Account Information

cafe24_mall_id = os.getenv('CAFE24_MALL_ID')
cafe24_client_id = os.getenv('CAFE24_CLIENT_ID')
cafe24_client_secret = os.getenv('CAFE24_CLIENT_SECRET')


st.write("# ì•„ì¹´ë¼ë¼ì´í”„ ìì‚¬ëª°! ğŸ‘‹")


# ê¸°ë³¸ ì¸ì¦ ì •ë³´ ìƒì„±
basic_auth = f"{cafe24_client_id}:{cafe24_client_secret}"
encoded_basic_auth = base64.b64encode(basic_auth.encode()).decode()

# ìš”ì²­ URL ì„¤ì •
url = f"https://{cafe24_mall_id}.cafe24api.com/api/v2/oauth/token"
headers = {
    'Authorization': f"Basic {encoded_basic_auth}",
    'Content-Type': 'application/x-www-form-urlencoded'
}

# refresh.csv íŒŒì¼ì—ì„œ refresh token ê°’ì„ ì½ì–´ì˜´
#with open('./refresh.csv', 'r') as file:
#    refresh_token = file.read().strip()



file_path = './pages/refresh.csv'
if os.path.isfile(file_path):
    print(file_path)
    with open(file_path, 'r') as file:
        refresh_token = file.read().strip()
else:
    print(f"File not found: {file_path}")



# ìš”ì²­ ë°ì´í„° ì„¤ì •
data = {
    'grant_type': 'refresh_token',
    'refresh_token': refresh_token
}

# POST ìš”ì²­ ë³´ë‚´ê¸°
response = requests.post(url, headers=headers, data=data)

# access_token ë° refresh_token ê°’ ì½ì–´ì˜¤ê¸°
if response.status_code == 200:
    response_data = response.json()
    access_token = response_data['access_token']
    refresh_token = response_data['refresh_token']
    print("Access Token:", access_token)
    print("Refresh Token:", refresh_token)
    print(response.json())
    # refresh tokenì„ CSV íŒŒì¼ì— ì €ì¥
    with open(file_path, 'w') as file:
        file.write(refresh_token)
else:
    print("Error:", response.text)






params_interval= st.slider("ê²€ìƒ‰ ë°ì´í„° ì¼ìˆ˜",min_value=5,max_value=30,step=1,value=10)# ì˜¤ëŠ˜ë¶€í„° ë©°ì¹  ì „ê¹Œì§€?
params_display = st.slider("ë³´ì—¬ì¤„ ë°ì´í„° ìˆ˜", min_value=1,max_value=10,step=1,value=5)
params_bulletin = st.selectbox("ê²Œì‹œíŒ ìœ í˜•",("Q&A","1:1","ìƒí’ˆí‰"))
bulletin= 6
if params_bulletin=="1:1":
    bulletin = 9
elif params_bulletin=="ìƒí’ˆí‰":
    bulletin = 4
    
payload = {}
files = {}
headers = {
    'Authorization': f'Bearer {access_token}',
    'X-Cafe24-Api-Version': '2024-03-01',
    'Content-Type': 'application/json',
    'Cookie': 'ECSESSID=5d169e847b0b49d2ff41047129114582'
}

url = f"https://aqarakr.cafe24api.com/api/v2/admin/boards/{bulletin}/articles"
# í˜„ì¬ ë‚ ì§œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
current_date = datetime.datetime.now()

# end_dateë¡œë¶€í„° 7ì¼ ì „ì˜ ë‚ ì§œë¥¼ êµ¬í•©ë‹ˆë‹¤.
start_date = current_date - datetime.timedelta(days=params_interval)

# ë‚ ì§œë¥¼ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
start_date_str = start_date.strftime('%Y-%m-%d')
end_date_str = current_date.strftime('%Y-%m-%d')

current_date = datetime.datetime.now().strftime('%Y-%m-%d')

params = {
    'start_date': start_date_str,
    'end_date': end_date_str,
    'limit':100
}

response = requests.request("GET", url, headers=headers, data=payload, files=files,params=params)
data = response.json()
articles_data = []
for article in data['articles']:
    # ê° ê¸°ì‚¬ì˜ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
    article_date = article['created_date']
    # í•´ë‹¹ ë‚ ì§œì˜ ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    content = article['content']
    filtered_text = BeautifulSoup(content, "html.parser").get_text()
    #st.write(f"Date: {article_date}")
    #st.write(filtered_text)
    articles_data.append({'registered_date': article_date, 'contents': filtered_text,'writer': article['writer']})


df = pd.DataFrame(articles_data)
st.table(df.head(params_display))


# Define the list of Korean device names to analyze
korean_device_names_to_analyze = ['M2','E1','í—ˆë¸Œ', 'ì„¼ì„œ', 'í”ŒëŸ¬ê·¸', 'ì½˜ì„¼íŠ¸', 'ì¡°ëª…ìŠ¤ìœ„ì¹˜', 'ë¸”ë¼ì¸ë“œ', 'ì»¤íŠ¼', 'ì¹´ë©”ë¼', 'ë„ì–´ë½','ì˜¨ìŠµë„','ì¡°ë„','ëˆ„ìˆ˜','ìŠ¤ìœ„ì¹˜','ë¬´ì„  ìŠ¤ìœ„ì¹˜','ìŠ¤ë§ˆíŠ¸ í”ŒëŸ¬ê·¸','í”ŒëŸ¬ê·¸']

# Extract Korean device names from the text using regular expressions
korean_device_names = []
if len(df)!=0:

    for text in df['contents']:
        for device in korean_device_names_to_analyze:
            matches = re.findall(r'\b{}\b'.format(device), text)
            korean_device_names.extend(matches)

# Count occurrences of each Korean device name
korean_word_count = Counter(korean_device_names)

# Generate the word cloud
if len(korean_word_count)!=0:
    korean_wordcloud = WordCloud(
        font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',  # í•œê¸€ í°íŠ¸ ê²½ë¡œ
        background_color='white',
        width=800,
        height=600
    ).generate_from_frequencies(korean_word_count)

    # í•œê¸€ í°íŠ¸ ì„¤ì •
    plt.rcParams['font.family'] = 'AppleGothic'  # Macì˜ ê²½ìš° AppleGothic, Windowsì˜ ê²½ìš° Malgun Gothic ë“±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

    # ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ëœ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    sorted_korean_word_count = korean_word_count.most_common()
    print(sorted_korean_word_count)
    print(df['contents'])

    # ì •ë ¬ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±
    fig, ax = plt.subplots(figsize=(10, 8))
    words, counts = zip(*sorted_korean_word_count)  # ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ë¥¼ ë¶„ë¦¬
    ax.bar(words, counts, color='skyblue')
    ax.set_xlabel('ë‹¨ì–´')
    ax.set_ylabel('ë¹ˆë„')
    ax.set_title('í•œê¸€ ë‹¨ì–´ ë¹ˆë„ (ë¹ˆë„ìˆ˜ ê¸°ì¤€ ì •ë ¬)')
    plt.xticks(rotation=45)

    # Streamlitì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ í‘œì‹œ
    st.write("### ê´€ì‹¬ ë””ë°”ì´ìŠ¤ í‚¤ì›Œë“œ ìˆ˜")
    st.pyplot(fig)
    st.write("### ê´€ì‹¬ ë””ë°”ì´ìŠ¤ ì›Œë“œ í´ë¼ìš°ë“œ")
    # Visualize the word cloud
    st.image(korean_wordcloud.to_array(), use_column_width=True)


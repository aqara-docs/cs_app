import streamlit as st
import time
from selenium import webdriver
import csv
import pandas as pd
from bs4 import BeautifulSoup as bs
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from konlpy.tag import Okt
from collections import Counter
import re
import os
from dotenv import load_dotenv
load_dotenv()

st.write("# ÏïÑÏπ¥Îùº Ïπ¥Ìéò! üëã")

class NaverCafeCrawler:
    def __init__(self, driver_path, url, id, pw, baseurl, clubid, userDisplay, boardType):
        self.total_list = ['Ï†úÎ™©', 'ÎÇ¥Ïö©', 'ÎßÅÌÅ¨']
        self.driver_path = driver_path
        self.url = url
        self.id = id
        self.pw = pw
        self.baseurl = baseurl
        self.baseraw = os.getenv('BASERAW')
        self.clubid = clubid
        self.userDisplay = userDisplay
        self.boardType = boardType

    def initialize_file(self, file_path='content.csv'):
        with open(file_path, 'w', encoding='utf-8', newline='') as f:
            wr = csv.writer(f)
            wr.writerow([self.total_list[0], self.total_list[1], self.total_list[2]])

    def login(self, browser):
        browser.get(self.url)
        browser.implicitly_wait(2)
        browser.execute_script(f"document.getElementsByName('id')[0].value='{self.id}'")
        browser.execute_script(f"document.getElementsByName('pw')[0].value='{self.pw}'")
        browser.find_element(By.XPATH, '//*[@id="log.login"]').click()
        time.sleep(1)

    def crawl_page(self, browser, page_num):
        browser.get(f"{self.baseurl}ArticleList.nhn?search.clubid={self.clubid}&userDisplay={self.userDisplay}"
                    f"&search.boardType={self.boardType}&search.page={page_num}")
        browser.switch_to.frame('cafe_main')
        soup = bs(browser.page_source, 'html.parser')
        soup = soup.find_all(class_='article-board m-tcol-c')[1]
        datas = soup.find_all(class_='td_article')
        new_df = pd.DataFrame(columns=['Ï†úÎ™©', 'ÎÇ¥Ïö©', 'ÎßÅÌÅ¨'])
        
        #print(datas)
        for data in datas:
            article_title = data.find(class_='article')
            link = article_title.get('href')
            article_title = article_title.get_text().strip()
            content = self.get_content(browser, self.baseraw + link)
            new_df = pd.concat([new_df, pd.DataFrame({'Ï†úÎ™©': [article_title], 'ÎÇ¥Ïö©': [content], 'ÎßÅÌÅ¨': [self.baseraw + link]})],
                              ignore_index=True)
        return new_df

    def get_content(self, browser, link):
        browser.get(link)
        time.sleep(1)
        browser.switch_to.frame('cafe_main')
        soup = bs(browser.page_source, 'html.parser')
        content = soup.find("div",{"class":"article_viewer"})
        
        if content:
            #print(f"Content: {content.get_text().strip()}")
            #print("\n")
            return content.get_text().strip()
        else:
            return ""

    def run(self, max_pages=2, file_path='content.csv'):
        self.initialize_file(file_path)
        offset = 0
        i = offset
        while i < max_pages+offset:
            i += 1
            pageNum = i
            print(f"Page Number: {pageNum}")
            original_df = pd.read_csv(file_path, encoding='utf-8')
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument(f'--webdriver-path={self.driver_path}')
            browser = webdriver.Chrome(options=chrome_options)
            self.login(browser)
            new_df = self.crawl_page(browser, pageNum)
            concat_df = pd.concat([original_df, new_df])
            concat_df = concat_df.drop_duplicates(keep=False)
            concat_df.to_csv(file_path, mode='a', header=False, index=False)
            browser.close()
            time.sleep(5)
        print("done completely....")

if __name__ == "__main__":
    driver_path = os.getenv('DRIVER_PATH')  # Update this path to your chromedriver location
    url = os.getenv('URL')
    id = os.getenv('ID')
    pw = os.getenv('PWD')
    baseurl = os.getenv('BASEURL')
    clubid = os.getenv('CLUBID')  # aqara
    userDisplay = 10
    boardType = 'L'
    params_pages = st.slider("Í≤åÏãúÌåê ÌéòÏù¥ÏßÄ Ïàò",min_value=1,max_value=100,step=1,value=1)
    params_display = st.slider("Î≥¥Ïó¨Ï§Ñ Îç∞Ïù¥ÌÑ∞ Ïàò", min_value=1,max_value=10,step=1,value=5)
    

    crawler = NaverCafeCrawler(driver_path, url, id, pw, baseurl, clubid, userDisplay, boardType)
    crawler.run(max_pages=params_pages)
    df = pd.read_csv("content.csv")
    st.table(df.loc[:,["Ï†úÎ™©","ÎÇ¥Ïö©"]].dropna().head(params_display))

    # Define the list of Korean device names to analyze
korean_device_names_to_analyze = ['Ïä§ÎßàÌä∏ Ïã±Ïä§','ÌôàÌÇ∑','Ïï†Ìîå ÌôàÌÇ∑','Îß§ÌÑ∞','matter','Matter','ÌóàÎ∏å', 'ÏÑºÏÑú', 'ÌîåÎü¨Í∑∏', 'ÏΩòÏÑºÌä∏', 'Ï°∞Î™ÖÏä§ÏúÑÏπò', 'Î∏îÎùºÏù∏Îìú', 'Ïª§Ìäº', 'Ïπ¥Î©îÎùº', 'ÎèÑÏñ¥ÎùΩ','Ïò®ÏäµÎèÑ','Ï°∞ÎèÑ','ÎàÑÏàò','Ïä§ÏúÑÏπò','Î¨¥ÏÑ† Ïä§ÏúÑÏπò']

# Extract Korean device names from the text using regular expressions
korean_device_names = []
print(df['ÎÇ¥Ïö©'])
if len(df.dropna())!=0:

    for text in df.dropna()['ÎÇ¥Ïö©']:
        for device in korean_device_names_to_analyze:
            matches = re.findall(r'\b{}\b'.format(device), text)
            korean_device_names.extend(matches)

# Count occurrences of each Korean device name
korean_word_count = Counter(korean_device_names)

# Generate the word cloud
if len(korean_word_count)!=0:
    korean_wordcloud = WordCloud(
        font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',  # ÌïúÍ∏Ä Ìè∞Ìä∏ Í≤ΩÎ°ú
        background_color='white',
        width=800,
        height=600
    ).generate_from_frequencies(korean_word_count)

    # ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï
    plt.rcParams['font.family'] = 'AppleGothic'  # MacÏùò Í≤ΩÏö∞ AppleGothic, WindowsÏùò Í≤ΩÏö∞ Malgun Gothic Îì±ÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.

    # ÎπàÎèÑÏàòÍ∞Ä ÎÜíÏùÄ ÏàúÏúºÎ°ú Ï†ïÎ†¨Îêú ÌäúÌîå Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±
    sorted_korean_word_count = korean_word_count.most_common()

    # Ï†ïÎ†¨Îêú Îç∞Ïù¥ÌÑ∞Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÎßâÎåÄ Í∑∏ÎûòÌîÑ ÏÉùÏÑ±
    fig, ax = plt.subplots(figsize=(10, 8))
    words, counts = zip(*sorted_korean_word_count)  # Îã®Ïñ¥ÏôÄ ÎπàÎèÑÏàòÎ•º Î∂ÑÎ¶¨
    ax.bar(words, counts, color='skyblue')
    ax.set_xlabel('Îã®Ïñ¥')
    ax.set_ylabel('ÎπàÎèÑ')
    ax.set_title('ÌïúÍ∏Ä Îã®Ïñ¥ ÎπàÎèÑ (ÎπàÎèÑÏàò Í∏∞Ï§Ä Ï†ïÎ†¨)')
    plt.xticks(rotation=45)

    # StreamlitÏóêÏÑú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù Ïù¥ÎØ∏ÏßÄÎ•º Base64Î°ú Ïù∏ÏΩîÎî©ÌïòÏó¨ ÌëúÏãú
    st.write("### Í¥ÄÏã¨ ÎîîÎ∞îÏù¥Ïä§ ÌÇ§ÏõåÎìú Ïàò")
    st.pyplot(fig)
    st.write("### Í¥ÄÏã¨ ÎîîÎ∞îÏù¥Ïä§ ÏõåÎìú ÌÅ¥ÎùºÏö∞Îìú")
    # Visualize the word cloud
    st.image(korean_wordcloud.to_array(), use_column_width=True)


